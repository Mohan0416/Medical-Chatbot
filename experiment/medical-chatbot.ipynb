{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages and libraries\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embdeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Data from PDF\n",
    "def load_pdf(data):\n",
    "  loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Text Chunks\n",
    "def text_split(extracted_data):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "  text_chunks = text_splitter.split_documents(extracted_data)\n",
    "  return text_chunks\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of the chunks: \", len(text_chunks)) # 7020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "  return embeddings\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Embedding:\n",
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length: \", len(query_result)) # 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pinecone Cluster\n",
    "# - Login into Pinecone: https://www.pinecone.io/ > Dashboard\n",
    "# - API Keys > Create API Key and use it\n",
    "# - Indexes > Create Index: medical-chatbot, 384 Dimensions, Cosine (HuggingFace Model: sentence-transformers/all-MiniLM-L6-v2) > Use the ENVIRONMENT\n",
    "\n",
    "# Initializing the PineCone\n",
    "PINECONE_API_KEY = \"...\"\n",
    "PINECONE_API_ENV = \"gcp-starter\"\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "index_name=\"medical-chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings for each of the text chunks and storing it\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Goto the Pinecone website, and view all the vectors that are stored. Total: 7020\n",
    "# - Now, performing semantic index: Doing Semantic or Similarity Search\n",
    "# If we already have an index we can load it like this\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "query = \"What are Allergies?\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else...\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "llm = CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\", model_type=\"llama\", config={\"max_new_tokens\":512, \"temperature\":0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Retrieval Question-Answering\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={'k':2}), return_source_documents=True, chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "while True:\n",
    "  user_input = input(f\"Input Prompt: \")\n",
    "  result = qa({\"query\": user_input})\n",
    "  print(\"Response: \", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
